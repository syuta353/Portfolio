import io
import os
import re
import json
import joblib
import numpy as np
import pandas as pd
import streamlit as st
from datetime import datetime

import joblib
import numpy as np
import pandas as pd
import streamlit as st
from datetime import datetime
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.ensemble import VotingClassifier, RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.neural_network import MLPClassifier

from data import DataManager

#ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–
data_manager = DataManager()


"""
0. DeviceModelTrainerï¼ˆè£…ç½®ç¨®åˆ¥ã”ã¨ã®æ‹¡å¼µãƒ¢ãƒ‡ãƒ«ç”Ÿæˆï¼‰
è²¬å‹™ï¼š

ãƒ»JSONãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã‚¨ãƒ©ãƒ¼ã‚³ãƒ¼ãƒ‰ã¨äº¤æ›éƒ¨å“ã®å¯¾å¿œè¡¨ã‚’ç”Ÿæˆ
ãƒ»è£…ç½®ç¨®åˆ¥ã”ã¨ã« VotingClassifier ã‚’å­¦ç¿’ã—ã€é–¢é€£ã™ã‚‹ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã‚„ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼ã¨ä¸€ç·’ã«ä¿å­˜

ä¸»ãªé–¢æ•°ï¼š

ãƒ»train_and_save_models()

"""
class DeviceModelTrainer:
    def __init__(self):
        self.json_path = data_manager.get_path("output_json")
        self.model_dir = data_manager.get_model_dir()
        os.makedirs(self.model_dir, exist_ok=True)

    def _load_json(self):
        with open(self.json_path, "r", encoding="utf-8") as f:
            return json.load(f)

    def _clean_text(self, text):
        if pd.isna(text):
            return ""
        return str(text).replace("\n", " ").replace("\t", " ").strip()

    def _extract_error_codes(self, text):
        return " ".join(re.findall(r"\b\d{7}\b", text))

    def _build_error_mapping(self, df):
        records = []
        for _, row in df.iterrows():
            error_text = str(row.get("éšœå®³ã‚¨ãƒ©ãƒ¼åæŠ½å‡º", ""))
            parts_text = str(row.get("äº¤æ›éƒ¨å“", ""))
            device_type = str(row.get("è£…ç½®ç¨®åˆ¥", ""))
            error_codes = set(re.findall(r"\b\d{7}\b", error_text))
            split_parts = str(parts_text).splitlines()
            split_parts = [p.strip() for p in split_parts if p.strip()]
            for code in error_codes:
                for part in split_parts:
                    records.append({
                        "ã‚¨ãƒ©ãƒ¼ã‚³ãƒ¼ãƒ‰": code,
                        "äº¤æ›éƒ¨å“": part,
                        "è£…ç½®ç¨®åˆ¥": device_type
                    })
        return pd.DataFrame(records)

    def train_and_save_models(self):
        with st.spinner("ğŸ¤– æ‹¡å¼µãƒ¢ãƒ‡ãƒ«ã‚’å†å­¦ç¿’ã—ã¦ã„ã¾ã™..."):
            json_data = self._load_json()
            df = pd.DataFrame(json_data)

            df["ä¸å…·åˆå†…å®¹_cleaned"] = df["ä¸å…·åˆå†…å®¹"].apply(self._clean_text)
            df["éšœå®³ã‚¨ãƒ©ãƒ¼åæŠ½å‡º"] = df["ä¸å…·åˆå†…å®¹_cleaned"].apply(self._extract_error_codes)

            error_df = self._build_error_mapping(df)

            for device in error_df["è£…ç½®ç¨®åˆ¥"].unique():
                subset = error_df[error_df["è£…ç½®ç¨®åˆ¥"] == device].copy()
                sample_counts = subset["äº¤æ›éƒ¨å“"].value_counts().to_dict()

                if len(subset["äº¤æ›éƒ¨å“"].unique()) < 2:
                    continue

                le_code = LabelEncoder()
                le_parts = LabelEncoder()
                subset["error_code_encoded"] = le_code.fit_transform(subset["ã‚¨ãƒ©ãƒ¼ã‚³ãƒ¼ãƒ‰"])
                subset["parts_encoded"] = le_parts.fit_transform(subset["äº¤æ›éƒ¨å“"])

                X = subset[["error_code_encoded"]]
                y = subset["parts_encoded"]

                scaler = StandardScaler()
                X_scaled = scaler.fit_transform(X)

                X_train, _, y_train, _ = train_test_split(X_scaled, y, test_size=0.3, random_state=42)
                knn_n = min(5, len(X_train))

                if len(set(y_train)) < 2:
                    continue

                model = VotingClassifier(
                    estimators=[
                        ('rf', RandomForestClassifier(random_state=42)),
                        ('lr', LogisticRegression(max_iter=1000, class_weight='balanced')),
                        ('knn', KNeighborsClassifier(n_neighbors=knn_n)),
                        ('mlp', MLPClassifier(hidden_layer_sizes=(100,), max_iter=1000, learning_rate_init=0.001))
                    ],
                    voting='soft'
                )
                model.fit(X_train, y_train)

                safe_device = re.sub(r"[\\/:*?\"<>|]", "_", device)
                joblib.dump(model, os.path.join(self.model_dir, f"model_{safe_device}.pkl"))
                joblib.dump(le_code, os.path.join(self.model_dir, f"le_code_{safe_device}.pkl"))
                joblib.dump(le_parts, os.path.join(self.model_dir, f"le_parts_{safe_device}.pkl"))
                joblib.dump(scaler, os.path.join(self.model_dir, f"scaler_{safe_device}.pkl"))
                joblib.dump(sample_counts, os.path.join(self.model_dir, f"sample_counts_{safe_device}.pkl"))

        st.success("âœ… æ‹¡å¼µãƒ¢ãƒ‡ãƒ«ã®å†å­¦ç¿’ã¨ä¿å­˜ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")

"""
1. DeviceModelManagerï¼ˆãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ãƒ»äºˆæ¸¬ï¼‰
è²¬å‹™ï¼š

ãƒ»ãƒ¢ãƒ‡ãƒ«ãƒ»ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ»ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼ã®èª­ã¿è¾¼ã¿
ãƒ»äº¤æ›éƒ¨å“ã®äºˆæ¸¬ã€é¸æŠã•ã‚ŒãŸè£…ç½®ç¨®åˆ¥ã®ä¿¡é ¼åº¦ã®è¡¨ç¤º
ãƒ»ã‚¸ãƒ£ãƒ³ãƒ—ã‚¹ã‚¿ãƒ¼ãƒˆè­¦å‘Šã®åˆ¤å®š

ä¸»ãªé–¢æ•°ï¼š

ãƒ»load_model(device_type: str)
ãƒ»predict_parts(error_code: str)
ãƒ»check_jumpstart_warning(error_code: str)

"""
class DeviceModelManager:
    def __init__(self):
        self.model = None
        self.le_code = None
        self.le_parts = None
        self.scaler = None
        self.sample_counts = None
        self.all_device_totals = None  # å…¨è£…ç½®ç¨®åˆ¥ã®ç·ä»¶æ•°ãƒªã‚¹ãƒˆ

    def load_model(self, device_type):
        model_dir = data_manager.get_model_dir()
        try:
            # ãƒ¢ãƒ‡ãƒ«ã¨é–¢é€£ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
            self.model = joblib.load(os.path.join(model_dir, f"model_{device_type}.pkl"))
            self.le_code = joblib.load(os.path.join(model_dir, f"le_code_{device_type}.pkl"))
            self.le_parts = joblib.load(os.path.join(model_dir, f"le_parts_{device_type}.pkl"))
            self.scaler = joblib.load(os.path.join(model_dir, f"scaler_{device_type}.pkl"))
            self.sample_counts = joblib.load(os.path.join(model_dir, f"sample_counts_{device_type}.pkl"))

            # å…¨è£…ç½®ç¨®åˆ¥ã®ç·ä»¶æ•°ã‚’å–å¾—
            self.all_device_totals = []
            for file in os.listdir(model_dir):
                if file.startswith("sample_counts_") and file.endswith(".pkl"):
                    counts = joblib.load(os.path.join(model_dir, file))
                    self.all_device_totals.extend(counts.values())
        except Exception as e:
            st.error(f"ãƒ¢ãƒ‡ãƒ«ã¾ãŸã¯é–¢é€£ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")

    def predict_parts(self, error_code):
        if error_code not in self.le_code.classes_:
            st.error("æŒ‡å®šã•ã‚ŒãŸã‚¨ãƒ©ãƒ¼ã‚³ãƒ¼ãƒ‰ã¯ã“ã®è£…ç½®ç¨®åˆ¥ã«å­˜åœ¨ã—ã¾ã›ã‚“ã€‚")
            return

        # ã‚¨ãƒ©ãƒ¼ã‚³ãƒ¼ãƒ‰ã‚’ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã—ã¦äºˆæ¸¬
        code_encoded = self.le_code.transform([error_code])
        X_input = self.scaler.transform(pd.DataFrame({"error_code_encoded": [code_encoded[0]]}))
        proba = self.model.predict_proba(X_input)[0]
        part_indices = proba.argsort()[::-1]
        
        
        # è£…ç½®ç¨®åˆ¥ã®ç·å­¦ç¿’ä»¶æ•°
        device_total = sum(self.sample_counts.values())
        
        # å…¨è£…ç½®ç¨®åˆ¥ã®ç·ä»¶æ•°ã®çµ±è¨ˆé‡ã‚’å–å¾—
        series = pd.Series(self.all_device_totals)
        stats = series.describe()
        
        # å…¨è£…ç½®ç¨®åˆ¥ã®åˆ†å¸ƒã‹ã‚‰ã—ãã„å€¤ã‚’æ±ºå®š
        #é«˜ã„æ•°å€¤ã®å¤–ã‚Œå€¤ã«å½±éŸ¿ã‚’å—ã‘ã¦ã„ã‚‹ã®ã§50%ä»¥ä¸‹ã‚’å°ã«ã™ã‚‹
        q50 = stats["50%"]
        q75 = stats["75%"]

        # è£…ç½®ç¨®åˆ¥ã®ä¿¡é ¼åº¦ãƒ©ãƒ™ãƒ«
        if device_total <= q50:
            device_confidence = "å°"
        elif device_total >= q75:
            device_confidence = "å¤§"
        else:
            device_confidence = "ä¸­"

        st.markdown("### ğŸ­ äºˆæ¸¬ã•ã‚Œã‚‹äº¤æ›éƒ¨å“ã®ä¿¡é ¼åº¦")
        st.write(f"è£…ç½®ç¨®åˆ¥ã®ç·å­¦ç¿’ä»¶æ•°: **{device_total} ä»¶**")
        st.write(f"ä¿¡é ¼åº¦: **{device_confidence}**")
        
        # äºˆæ¸¬çµæœã®è¡¨ç¤º
        st.markdown("### ğŸ” äºˆæ¸¬ã•ã‚Œã‚‹äº¤æ›éƒ¨å“ã¨ãã®ç¢ºç‡")
        predicted_parts = []
        probabilities = []

        for idx in part_indices:
            percent = round(proba[idx] * 100, 2)
            if percent > 0.0:
                part_name = self.le_parts.inverse_transform([idx])[0]
                st.write(f"- {part_name}: {percent}%")
                
        st.markdown("### â„¹ï¸ ä¿¡é ¼åº¦ã«ã¤ã„ã¦ã®èª¬æ˜")
        st.markdown("""
        ä¿¡é ¼åº¦ã¯ã€**ã“ã®è£…ç½®ç¨®åˆ¥ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã®é‡ã«åŸºã¥ã„ã¦ã€äºˆæ¸¬çµæœã®ç¢ºã‹ã•ã‚’ç¤ºã™æŒ‡æ¨™**ã§ã™ã€‚
        å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ãŒå¤šã„ã»ã©ã€ãƒ¢ãƒ‡ãƒ«ã¯ã‚ˆã‚Šå¤šãã®äº‹ä¾‹ã‚’å­¦ã‚“ã§ã„ã‚‹ãŸã‚ã€äºˆæ¸¬ã®ç²¾åº¦ãŒé«˜ããªã‚‹å‚¾å‘ãŒã‚ã‚Šã¾ã™ã€‚
        é€†ã«ã€å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ãŒå°‘ãªã„å ´åˆã¯ã€äºˆæ¸¬ã®ç¢ºã‹ã•ãŒä½ããªã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚

        - **å¤§**ï¼šå­¦ç¿’ãƒ‡ãƒ¼ã‚¿ãŒå¤šãã€äºˆæ¸¬ã®ä¿¡é ¼æ€§ãŒé«˜ã„
        - **ä¸­**ï¼šå­¦ç¿’ãƒ‡ãƒ¼ã‚¿ãŒæ¨™æº–çš„ã§ã€äºˆæ¸¬ã®ä¿¡é ¼æ€§ã¯æ™®é€š
        - **å°**ï¼šå­¦ç¿’ãƒ‡ãƒ¼ã‚¿ãŒå°‘ãªãã€äºˆæ¸¬ã®ä¿¡é ¼æ€§ãŒä½ã„å¯èƒ½æ€§ãŒã‚ã‚‹
        """)
        
    # è­¦å‘Šè¡¨ç¤ºå‡¦ç†
    def check_jumpstart_warning(self, error_code, device_type):
        """
        æŒ‡å®šã•ã‚ŒãŸã‚¨ãƒ©ãƒ¼ã‚³ãƒ¼ãƒ‰ãŒã‚¸ãƒ£ãƒ³ãƒ—ã‚¹ã‚¿ãƒ¼ãƒˆå¯¾è±¡ã‹ã©ã†ã‹ã¨
        æŒ‡å®šã•ã‚ŒãŸè£…ç½®ç¨®åˆ¥ã‚’ã‚¸ãƒ£ãƒ³ãƒ—ã‚¹ã‚¿ãƒ¼ãƒˆå¯¾è±¡ã®è£…ç½®ç¨®åˆ¥ã‹ã©ã†ã‹ã‚’åˆ¤å®šã—ã€è­¦å‘Šã‚’è¡¨ç¤ºã—ã¾ã™ã€‚

        å¼•æ•°:
        - error_code (str): ã‚¨ãƒ©ãƒ¼ã‚³ãƒ¼ãƒ‰ã€‚
        - device_type (str): è£…ç½®ç¨®åˆ¥ã€‚

        æˆ»ã‚Šå€¤:
        - ãªã—ï¼ˆStreamlitä¸Šã«è­¦å‘Šè¡¨ç¤ºï¼‰
        """
        jump_errors = data_manager.get_device_jump_errors()
        
        if device_type in jump_errors:
            if error_code in jump_errors[device_type]:
                st.warning(data_manager.jump_warning)
        
        if device_type in ["device_type1", "device_type2", "device_type3"]:
            st.warning(data_manager.route_warning)

"""
2. FailureDataProcessorï¼ˆExcelâ†’JSONå¤‰æ›ã€éƒ¨å“åæ­£è¦åŒ–ï¼‰
è²¬å‹™ï¼š

ãƒ»Excelã‚·ãƒ¼ãƒˆã®èª­ã¿è¾¼ã¿ã¨æ•´å½¢
ãƒ»éƒ¨å“åã®æ­£è¦åŒ–
ãƒ»JSONä¿å­˜
ãƒ»# Excelå‡ºåŠ›

ä¸»ãªé–¢æ•°ï¼š

ãƒ»process_sheet(sheet_name, file_path)
ãƒ»convert_excel_to_json(file_path)
ãƒ»clean_part_name(part, counter_text)
ãƒ»normalize_parts(part)
ãƒ»save_to_json(df, path)
ãƒ»export_excel

"""
class FailureDataProcessor:
    def __init__(self):
        self.rear_word, self.front_word = data_manager.get_keywords()
        self.host_df, self.type_df, self.bill_df = data_manager.get_dataframes()
        self.missing_list = data_manager.get_missing_list()

    def process_sheet(self, sheet_name, file_path):
        """
        æŒ‡å®šã•ã‚ŒãŸExcelã‚·ãƒ¼ãƒˆã‚’èª­ã¿è¾¼ã¿ã€æ§‹é€ åŒ–ã•ã‚ŒãŸæ•…éšœãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡ºã—ã¾ã™ã€‚

        å¼•æ•°:
        - sheet_name (str): å‡¦ç†å¯¾è±¡ã®ã‚·ãƒ¼ãƒˆåã€‚
        - file_path (str): Excelãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã€‚

        æˆ»ã‚Šå€¤:
        - list: æ•…éšœãƒ‡ãƒ¼ã‚¿ã®è¾æ›¸ãƒªã‚¹ãƒˆã€‚
        """
        output_data = []
        try:
            df = pd.read_excel(
                file_path,
                sheet_name=sheet_name,
                header=1,
                usecols=["ç™ºç”Ÿæ—¥", "ç™ºç”Ÿå±€", "è£…ç½®ç¨®åˆ¥", "è£…ç½®å(host)", "ä¸å…·åˆå†…å®¹", "ç¾åœ°å¯¾å¿œéƒ¨é–€ã§ã®å¯¾ç­–ï¼ˆå‡¦ç½®ï¼‰", "ã‚ªãƒ³ã‚µã‚¤ãƒˆäº¤æ›éƒ¨å“"],
                engine='openpyxl'
            )
            df = df.rename(columns={
                'ç™ºç”Ÿå±€': 'ãƒ“ãƒ«å',
                'è£…ç½®å(host)': 'è£…ç½®å',
                'ç¾åœ°å¯¾å¿œéƒ¨é–€ã§ã®å¯¾ç­–ï¼ˆå‡¦ç½®ï¼‰': 'å¯¾ç­–å†…å®¹',
                'ã‚ªãƒ³ã‚µã‚¤ãƒˆäº¤æ›éƒ¨å“': 'äº¤æ›éƒ¨å“'
            })
            df['ç™ºç”Ÿæ—¥'] = pd.to_datetime(df['ç™ºç”Ÿæ—¥'], errors='coerce')
            df['äº¤æ›éƒ¨å“'] = df['äº¤æ›éƒ¨å“'].apply(self.normalize_parts)
            df = df.replace("-", pd.NA).replace("ä¸æ˜", pd.NA).replace("", pd.NA)
            df = df.where(pd.notna(df), None)
            df = df.dropna(subset=['ç™ºç”Ÿæ—¥', 'ãƒ“ãƒ«å', 'è£…ç½®ç¨®åˆ¥', 'è£…ç½®å', 'äº¤æ›éƒ¨å“'])

            resolver = HostInfoResolver()

            for _, row in df.iterrows():
                for dname in str(row['è£…ç½®å']).splitlines():
                    dname = dname.strip()
                    result = resolver.get_info(dname)
                    if result is None:
                        continue
                    bill, type_ = result
                    original_part = str(row['äº¤æ›éƒ¨å“'])
                    counter_text = row['å¯¾ç­–å†…å®¹']
                    part_type = self.clean_part_name(original_part, counter_text)
                    if part_type in ["target_type1", "target_type2", "target_type3"] and part_type != type_:
                        continue
                    output_data.append({
                        "ç™ºç”Ÿæ—¥": row['ç™ºç”Ÿæ—¥'].strftime('%Y-%m-%d'),
                        "ãƒ“ãƒ«å": bill,
                        "è£…ç½®ç¨®åˆ¥": type_,
                        "è£…ç½®å": dname,
                        "ä¸å…·åˆå†…å®¹": row['ä¸å…·åˆå†…å®¹'],
                        "å¯¾ç­–å†…å®¹": row['å¯¾ç­–å†…å®¹'],
                        "äº¤æ›éƒ¨å“": part_type,
                        "å¹´åº¦": row['ç™ºç”Ÿæ—¥'].strftime('%Y')
                    })
        except Exception as e:
            print(f"{sheet_name} ã®èª­ã¿è¾¼ã¿ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return output_data

    def convert_excel_to_json(self, file_path):
        """
        Excelãƒ•ã‚¡ã‚¤ãƒ«å†…ã®è¤‡æ•°ã‚·ãƒ¼ãƒˆã‚’èª­ã¿è¾¼ã¿ã€1ã¤ã®JSONãƒ•ã‚¡ã‚¤ãƒ«ã«å¤‰æ›ã—ã¦ä¿å­˜ã—ã¾ã™ã€‚

        å¼•æ•°:
        - file_path (str): Excelãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã€‚

        æˆ»ã‚Šå€¤:
        - ãªã—
        """
        with st.spinner("ğŸ”„ JSONå¤‰æ›ä¸­ã§ã™ã€‚ã—ã°ã‚‰ããŠå¾…ã¡ãã ã•ã„..."):
            all_output_data = []
            for sheet in data_manager.get_target_sheets():
                all_output_data.extend(self.process_sheet(sheet, file_path))
            with open(data_manager.get_path("output_json"), 'w', encoding='utf-8') as f:
                json.dump(all_output_data, f, ensure_ascii=False, indent=2)
            with open(data_manager.get_path("missing_list"), 'w', encoding='utf-8') as f:
                for item in self.missing_list:
                    f.write(item + '\n')
                self.missing_list.clear()

    def clean_part_name(self, part, counter_text):
        """
        äº¤æ›éƒ¨å“åã‚’ãƒ«ãƒ¼ãƒ«ã«å¾“ã£ã¦æ­£è¦åŒ–ã™ã‚‹é–¢æ•°ã€‚

        å¼•æ•°:
        - part (str): å…ƒã®äº¤æ›éƒ¨å“å
        - countermeasure_text (str): å¯¾ç­–å†…å®¹
        - rear_word (list): ãƒªã‚¢åˆ¤å®šç”¨ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒªã‚¹ãƒˆ
        - front_word (list): ãƒ•ãƒ­ãƒ³ãƒˆåˆ¤å®šç”¨ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒªã‚¹ãƒˆ

        æˆ»ã‚Šå€¤:
        - str: æ­£è¦åŒ–ã•ã‚ŒãŸäº¤æ›éƒ¨å“å
        """
        if not isinstance(part, str):
            return part
        part_lower = part.lower()
        if "target_part" in part_lower:
            text = str(counter_text).lower()
            is_rear = any(word.lower() in text for word in self.rear_word)
            is_front = any(word.lower() in text for word in self.front_word)
            if is_rear and is_front:
                part = "replace_part"
            elif is_rear:
                part = "replace_part"
            elif is_front:
                part = "replace_part"
            else:
                part = "replace_part"
        if "target_part" in part_lower or "target_part" in part:
            part = "replace_part"
        elif "target_part" in part:
            part = "replace_part"
        elif "target_part" in part_lower:
            part = "replace_part"
        elif "target_part" in part_lower:
            part = "replace_part"
        part = re.sub(r"\(.*?\)", "", part)
        part = re.sub(r"â€».*", "", part)
        return part.strip()

    def normalize_parts(self, part):
        """
        éƒ¨å“åã®è¡¨è¨˜ã‚†ã‚Œã‚’æ­£è¦åŒ–ã—ã¾ã™ã€‚

        å¼•æ•°:
        - part (str): å…ƒã®éƒ¨å“åã€‚

        æˆ»ã‚Šå€¤:
        - str: æ­£è¦åŒ–ã•ã‚ŒãŸéƒ¨å“åã€‚
        """
        if pd.isna(part):
            return part
        part = str(part)
        part = part.replace("-", "çµŒéè¦³å¯Ÿ/HWç•°å¸¸ãªã—")
        part = part.replace("target_part1", "ä¸æ˜")
        part = re.sub(r"target_part2", "ä¸æ˜", part)
        part = re.sub(r"target_part3", "ä¸æ˜", part)
        return part

    def save_to_json(self, df, path):
        """
        DataFrameã‚’æ•´å½¢ã•ã‚ŒãŸJSONãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜ã—ã¾ã™ã€‚

        å¼•æ•°:
        - df (pd.DataFrame): ä¿å­˜å¯¾è±¡ã®ãƒ‡ãƒ¼ã‚¿ã€‚
        - json_path (str): ä¿å­˜å…ˆã®ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã€‚

        æˆ»ã‚Šå€¤:
        - ãªã—
        """
        df = df.fillna("")
        df["id"] = df["id"].apply(lambda x: str(int(x)) if pd.notnull(x) else "")
        json_data = df.to_json(orient='records', force_ascii=False)
        json_obj = json.loads(json_data)
        json_str = json.dumps(json_obj, indent=4, ensure_ascii=False)
        with open(path, 'w', encoding='utf-8') as f:
            f.write(json_str)
            
    def export_excel(self, df, sheet_name, file_name, button_label="ğŸ“¥ Excelãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰"):
        """
        DataFrameã‚’Excelãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦å‡ºåŠ›ã—ã€Streamlitä¸Šã«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒœã‚¿ãƒ³ã‚’è¡¨ç¤ºã—ã¾ã™ã€‚

        å¼•æ•°:
        - df (pd.DataFrame): å‡ºåŠ›å¯¾è±¡ã®ãƒ‡ãƒ¼ã‚¿ã€‚
        - sheet_name (str): Excelã‚·ãƒ¼ãƒˆåã€‚
        - file_name (str): ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ™‚ã®ãƒ•ã‚¡ã‚¤ãƒ«åã€‚
        - button_label (str): ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒœã‚¿ãƒ³ã®ãƒ©ãƒ™ãƒ«ã€‚

        æˆ»ã‚Šå€¤:
        - ãªã—
        """
        buffer = io.BytesIO()
        with pd.ExcelWriter(buffer, engine='openpyxl') as writer:
            df.to_excel(writer, sheet_name=sheet_name, index=False)
        st.download_button(
            label=button_label,
            data=buffer.getvalue(),
            file_name=file_name,
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
        )
        
"""
3. AnomalyDetectorï¼ˆç•°å¸¸æ¤œçŸ¥ï¼‰
è²¬å‹™ï¼š

ãƒ»æ•…éšœä»¶æ•°ã®é›†è¨ˆ
ãƒ»ç•°å¸¸åˆ¤å®š
ãƒ»çµ±è¨ˆæƒ…å ±ã®ç®—å‡º

ä¸»ãªé–¢æ•°ï¼š

ãƒ»detect_anomalies(df, type_counts, threshold, unit, year)
ãƒ»count_anomalies_by_type(type_counts, threshold)
ãƒ»calculate_failure_statistics(df, selected_years)

"""
class AnomalyDetector:
    def detect_anomalies(self, df, type_counts, threshold_percent, aggregation_unit, selected_year=None):
        """
        æ•…éšœä»¶æ•°ã«åŸºã¥ã„ã¦ç•°å¸¸ã‚’æ¤œçŸ¥ã—ã€ç•°å¸¸ãƒ»æ­£å¸¸ã®ãƒ©ãƒ™ãƒ«ã‚’ä»˜ä¸ã—ã¾ã™ã€‚

        å¼•æ•°:
        - df (DataFrame): æ•…éšœãƒ‡ãƒ¼ã‚¿ã€‚
        - type_counts (DataFrame): è£…ç½®ç¨®åˆ¥ã”ã¨ã®æ¯æ•°ã€‚
        - threshold_percent (float): ç•°å¸¸åˆ¤å®šã®ã—ãã„å€¤ï¼ˆ%ï¼‰ã€‚
        - aggregation_unit (str): é›†è¨ˆå˜ä½ï¼ˆå¹´åº¦å˜ä½ã¾ãŸã¯æœˆé–“å˜ä½ï¼‰ã€‚
        - selected_year (int, optional): æŒ‡å®šå¹´åº¦ï¼ˆå¿…è¦ã«å¿œã˜ã¦ï¼‰ã€‚

        æˆ»ã‚Šå€¤:
        - DataFrame: ç•°å¸¸åˆ¤å®šçµæœã‚’å«ã‚€ãƒ‡ãƒ¼ã‚¿ã€‚
        """
        threshold = threshold_percent / 100
        anomaly_results = []
        df['ç™ºç”Ÿæ—¥'] = pd.to_datetime(df['ç™ºç”Ÿæ—¥'], errors='coerce').dt.strftime('%Y-%m-%d')

        for dtype in df['è£…ç½®ç¨®åˆ¥'].dropna().unique():
            sub_df = df[df['è£…ç½®ç¨®åˆ¥'] == dtype]

            if aggregation_unit == "æŒ‡å®šå¹´åº¦ã®æœˆé–“å˜ä½":
                if selected_year is None:
                    continue
                sub_df = sub_df[sub_df['å¹´åº¦'] == selected_year]

            grouped = sub_df.groupby(['ç™ºç”Ÿæ—¥', 'å¹´åº¦', 'æœˆ']).size().reset_index(name='ä»¶æ•°')
            grouped['å¹´åº¦'] = selected_year if selected_year else grouped['å¹´åº¦']

            if len(grouped) < 1:
                continue

            mother_count = type_counts.query("d_type_name == @dtype")['count'].sum()
            if mother_count == 0:
                continue

            grouped['è£…ç½®ç¨®åˆ¥'] = dtype
            grouped['ç•°å¸¸'] = (grouped['ä»¶æ•°'] / mother_count) > threshold
            grouped['ç•°å¸¸'] = grouped['ç•°å¸¸'].apply(lambda x: 'ç•°å¸¸' if x else 'æ­£å¸¸')

            anomaly_results.append(grouped)

        if anomaly_results:
            result_df = pd.concat(anomaly_results, ignore_index=True)
            result_df = result_df.sort_values(by='ç™ºç”Ÿæ—¥', ascending=True).reset_index(drop=True)
            return result_df
        else:
            return pd.DataFrame()

    def count_anomalies_by_type(self, type_counts, threshold_percent):
        """
        è£…ç½®ç¨®åˆ¥ã”ã¨ã®ã—ãã„å€¤ä»¶æ•°ã‚’è¨ˆç®—ã™ã‚‹é–¢æ•°ã€‚
        Parameters:
            type_counts (pd.DataFrame): è£…ç½®ç¨®åˆ¥ã”ã¨ã®å…¨ä½“æ•°ã‚’å«ã‚€ DataFrameï¼ˆåˆ—: 'd_type_name', 'count'ï¼‰
            threshold_percent (float): é–¾å€¤ï¼ˆï¼…ï¼‰
        Returns:
            pd.DataFrame: ['è£…ç½®ç¨®åˆ¥', 'å…¨ä½“æ•°', 'ã—ãã„å€¤ä»¶æ•°'] ã‚’å«ã‚€ DataFrame
        """
        threshold_ratio = threshold_percent / 100.0
        result_df = type_counts.copy()
        result_df = result_df.rename(columns={'d_type_name': 'è£…ç½®ç¨®åˆ¥', 'count': 'å…¨ä½“æ•°'})
        result_df['ã—ãã„å€¤ä»¶æ•°'] = (result_df['å…¨ä½“æ•°'] * threshold_ratio).round().astype(int)
        return result_df

    def calculate_failure_statistics(self, df, selected_years):
        """
        æ•…éšœä»¶æ•°ã®çµ±è¨ˆé‡ï¼ˆå¹³å‡ãƒ»ä¸­å¤®å€¤ãƒ»æœ€å°ãƒ»æœ€å¤§ãƒ»æ¨™æº–åå·®ï¼‰ã¨å‚¾å‘èª¬æ˜ã‚’ç®—å‡ºã—ã¾ã™ã€‚

        å¼•æ•°:
        - df (DataFrame): æ•…éšœãƒ‡ãƒ¼ã‚¿ã€‚
        - selected_years (list): å¯¾è±¡å¹´åº¦ã®ãƒªã‚¹ãƒˆã€‚

        æˆ»ã‚Šå€¤:
        - dict: çµ±è¨ˆé‡ã¨å‚¾å‘èª¬æ˜ã‚’å«ã‚€è¾æ›¸ã€‚
        """
        if not selected_years:
            return {
                "mean": 0, "median": 0, "min": 0, "max": 0, "std": 0,
                "trend_description": "å¹´åº¦ãŒé¸æŠã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚",
                "label": "ï¼ˆå¹´åº¦æœªé¸æŠï¼‰"
            }

        if len(selected_years) == 1:
            year = selected_years[0]
            values = df[df['å¹´åº¦'] == year]['æœˆ'].value_counts().sort_index().values
            label = f"{year}å¹´ï¼ˆæœˆåˆ¥ï¼‰"
            mode = "æœˆ"
            thresholds = [5, 15]
        else:
            values = df[df['å¹´åº¦'].isin(selected_years)]['å¹´åº¦'].value_counts().sort_index().values
            min_year, max_year = min(selected_years), max(selected_years)
            label = f"{min_year}ã€œ{max_year}å¹´" if min_year != max_year else f"{min_year}å¹´"
            mode = "å¹´åº¦"
            thresholds = [10, 30]

        if len(values) == 0:
            stats = {"mean": 0, "median": 0, "min": 0, "max": 0, "std": 0}
        else:
            stats = {
                "mean": int(np.nanmean(values)),
                "median": int(np.nanmedian(values)),
                "min": int(np.nanmin(values)),
                "max": int(np.nanmax(values)),
                "std": int(np.nanstd(values))
            }

        std_val = stats["std"]
        if std_val == 0:
            trend_description = f"{mode}ã”ã¨ã®æ•…éšœä»¶æ•°ã¯ã»ã¼ä¸€å®šã§ã€ã°ã‚‰ã¤ããŒã‚ã‚Šã¾ã›ã‚“ã€‚"
        elif std_val < thresholds[0]:
            trend_description = f"{mode}ã”ã¨ã®æ•…éšœä»¶æ•°ã¯æ¯”è¼ƒçš„å®‰å®šã—ã¦ãŠã‚Šã€å¤§ããªå¤‰å‹•ã¯è¦‹ã‚‰ã‚Œã¾ã›ã‚“ã€‚"
        elif std_val < thresholds[1]:
            trend_description = f"{mode}ã”ã¨ã®æ•…éšœä»¶æ•°ã«ã¯ä¸­ç¨‹åº¦ã®ã°ã‚‰ã¤ããŒã‚ã‚Šã€{mode}ã«ã‚ˆã£ã¦å¤šå°‘ã®é•ã„ãŒã‚ã‚Šã¾ã™ã€‚"
        else:
            trend_description = f"{mode}ã”ã¨ã®æ•…éšœä»¶æ•°ã«ã¯å¤§ããªã°ã‚‰ã¤ããŒã‚ã‚Šã€{mode}ã«ã‚ˆã£ã¦æ•…éšœä»¶æ•°ã«å¤§ããªé•ã„ãŒã‚ã‚Šã¾ã™ã€‚"

        return {
            **stats,
            "trend_description": trend_description,
            "label": label
        }

"""
4. HostInfoResolverï¼ˆãƒ›ã‚¹ãƒˆåâ†’è£…ç½®ç¨®åˆ¥ãƒ»ãƒ“ãƒ«åï¼‰
è²¬å‹™ï¼š

ãƒ»ãƒ›ã‚¹ãƒˆåã‹ã‚‰é–¢é€£æƒ…å ±ã‚’å–å¾—
ãƒ»æ¬ æãƒªã‚¹ãƒˆã®ç®¡ç†

ä¸»ãªé–¢æ•°ï¼š

ãƒ»get_info(hostname)
ãƒ»aggregate_counts()

"""
class HostInfoResolver:
    def __init__(self):
        self.host_df, self.type_df, self.bill_df = data_manager.get_dataframes()
        self.missing_list = data_manager.get_missing_list()

    def get_info(self, hostname):
        
        """
        æŒ‡å®šã•ã‚ŒãŸãƒ›ã‚¹ãƒˆåã«å¯¾å¿œã™ã‚‹ãƒ“ãƒ«åã¨è£…ç½®ç¨®åˆ¥ã‚’å–å¾—ã—ã¾ã™ã€‚
        æ¬ æãŒã‚ã‚‹å ´åˆã¯ missing_list ã«æ—¥æœ¬èªãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¿½åŠ ã—ã¾ã™ã€‚

        å¼•æ•°:
        - hostName (str): å¯¾è±¡ã®ãƒ›ã‚¹ãƒˆåã€‚
        - host_df (pd.DataFrame): ãƒ›ã‚¹ãƒˆæƒ…å ±ã€‚
        - bill_df (pd.DataFrame): ãƒ“ãƒ«æƒ…å ±ã€‚
        - type_df (pd.DataFrame): è£…ç½®ç¨®åˆ¥æƒ…å ±ã€‚
        - missing_list (list): æ¬ æãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¨˜éŒ²ã™ã‚‹ãƒªã‚¹ãƒˆã€‚

        æˆ»ã‚Šå€¤:
        - tuple: (ãƒ“ãƒ«å, è£…ç½®ç¨®åˆ¥)ã€‚è©²å½“ãªã—ã®å ´åˆã¯ Noneã€‚
        """
        data = self.host_df.loc[self.host_df['hostname'] == hostname]
        if data.empty:
            self.missing_list.append(f"ãƒ›ã‚¹ãƒˆå '{hostname}' ã«è©²å½“ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
            return None

        data = data.iloc[0]

        bill_match = self.bill_df.loc[self.bill_df['key'] == data['bill']]
        bill_name = bill_match['name'].values[0] if not bill_match.empty else "ä¸æ˜"
        if bill_match.empty:
            self.missing_list.append(f"ãƒ“ãƒ«å '{data['bill']}' ã«è©²å½“ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")

        if data['d_type'] not in self.type_df:
            d_type = "ä¸æ˜"
            self.missing_list.append(f"è£…ç½®ç¨®åˆ¥ '{data['d_type']}' ã«è©²å½“ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        else:
            d_type = self.type_df[data['d_type']][0]

        return bill_name, d_type

    def aggregate_counts(self):
        """
        ãƒ›ã‚¹ãƒˆæƒ…å ±ã‹ã‚‰ãƒ“ãƒ«åã¨è£…ç½®ç¨®åˆ¥ã®å‡ºç¾å›æ•°ã‚’é›†è¨ˆã—ã¾ã™ã€‚

        å¼•æ•°:
        - host_df (pd.DataFrame): ãƒ›ã‚¹ãƒˆæƒ…å ±ã€‚
        - bill_df (pd.DataFrame): ãƒ“ãƒ«æƒ…å ±ã€‚
        - type_df (pd.DataFrame): è£…ç½®ç¨®åˆ¥æƒ…å ±ã€‚

        æˆ»ã‚Šå€¤:
        - tuple: (ãƒ“ãƒ«åˆ¥ä»¶æ•°ã®DataFrame, è£…ç½®ç¨®åˆ¥åˆ¥ä»¶æ•°ã®DataFrame)
        """
        results = []
        for hostname in self.host_df['hostname']:
            result = self.get_info(hostname)
            if result is not None:
                bill_name, d_type_name = result
                results.append({'bill_name': bill_name, 'd_type_name': d_type_name})

        results_df = pd.DataFrame(results)
        bill_counts = results_df.groupby('bill_name').size().reset_index(name='count')
        type_counts = results_df.groupby('d_type_name').size().reset_index(name='count')
        return bill_counts, type_counts
        

"""
5. ThemeManagerï¼ˆUIãƒ†ãƒ¼ãƒé©ç”¨ï¼‰
è²¬å‹™ï¼š

ãƒ»Streamlitã®CSSãƒ†ãƒ¼ãƒé©ç”¨

ä¸»ãªé–¢æ•°ï¼š

ãƒ»apply_theme(theme_name: str)

"""
class ThemeManager:
    def apply_theme(self, theme_name: str):
        """
        é¸æŠã•ã‚ŒãŸãƒ†ãƒ¼ãƒã«å¿œã˜ã¦Streamlitã‚¢ãƒ—ãƒªã®CSSã‚¹ã‚¿ã‚¤ãƒ«ã‚’é©ç”¨ã—ã¾ã™ã€‚

        å¼•æ•°:
        - theme_option (str): ãƒ†ãƒ¼ãƒåï¼ˆãƒ©ã‚¤ãƒˆã€ãƒ€ãƒ¼ã‚¯ãªã©ï¼‰ã€‚

        æˆ»ã‚Šå€¤:
        - ãªã—
        """
        themes = {
            "ãƒ©ã‚¤ãƒˆ": {
                "bg_color": "#ffffff", "text_color": "#000000", "app_bg": "#f8f8f8",
                "metric_bg": "#e6f2ff", "button_bg": "#e0e0e0", "tab_bg": "#e6f2ff",
                "tab_selected_bg": "#cce0ff", "expander_bg": "#e6f2ff", "expander_border": "#cce0ff",
                "sidebar_bg": "#f0f0f0", "dataframe_bg": "#ffffff"
            },
            "ãƒ€ãƒ¼ã‚¯": {
                "bg_color": "#1e1e1e", "text_color": "#ffffff", "app_bg": "#2e2e2e",
                "metric_bg": "#333333", "button_bg": "#444444", "tab_bg": "#444444",
                "tab_selected_bg": "#666666", "expander_bg": "#333333", "expander_border": "#666666",
                "sidebar_bg": "#2e2e2e", "dataframe_bg": "#333333"
            },
            "ãƒ–ãƒ«ãƒ¼": {
                "bg_color": "#e6f0ff", "text_color": "#003366", "app_bg": "#d9e6f2",
                "metric_bg": "#cce0ff", "button_bg": "#99ccff", "tab_bg": "#cce0ff",
                "tab_selected_bg": "#99ccff", "expander_bg": "#cce0ff", "expander_border": "#99ccff",
                "sidebar_bg": "#b3d1ff", "dataframe_bg": "#ffffff"
            },
            "ã‚°ãƒªãƒ¼ãƒ³": {
                "bg_color": "#e6ffe6", "text_color": "#004d00", "app_bg": "#ccffcc",
                "metric_bg": "#b3ffb3", "button_bg": "#80ff80", "tab_bg": "#b3ffb3",
                "tab_selected_bg": "#80ff80", "expander_bg": "#b3ffb3", "expander_border": "#80ff80",
                "sidebar_bg": "#99ff99", "dataframe_bg": "#ffffff"
            }
        }

        if theme_name not in themes:
            return

        t = themes[theme_name]
        st.markdown(f"""
        <style>
        body {{ background-color: {t['bg_color']}; color: {t['text_color']}; }}
        .stApp {{ background-color: {t['app_bg']}; }}
        h1, h2, h3, h4, h5, h6, p, .stMarkdown {{ color: {t['text_color']} !important; }}
        .stMetric {{ background-color: {t['metric_bg']}; padding: 10px; border-radius: 5px; }}
        .stMetric label {{ color: {t['text_color']} !important; font-weight: bold; }}
        .stMetric div {{ color: {t['text_color']} !important; }}
        .stButton>button, .stDownloadButton>button {{
            background-color: {t['button_bg']};
            color: {t['text_color']};
            font-weight: bold;
            border-radius: 5px;
            padding: 6px 12px;
        }}
        label[data-testid="stSelectboxLabel"] {{
            color: {t['text_color']} !important;
            font-weight: bold;
        }}
        .stDataFrame {{ background-color: {t['dataframe_bg']}; color: {t['text_color']}; }}
        .stSidebar {{ background-color: {t['sidebar_bg']}; }}
        .stTabs [data-baseweb="tab"] {{
            background-color: {t['tab_bg']};
            color: {t['text_color']};
            border-radius: 5px;
            padding: 6px;
            margin-right: 4px;
        }}
        .stTabs [aria-selected="true"] {{
            background-color: {t['tab_selected_bg']};
            font-weight: bold;
        }}
        .stExpander {{
            background-color: {t['expander_bg']} !important;
            color: {t['text_color']} !important;
            border: 1px solid {t['expander_border']};
            border-radius: 5px;
        }}
        .stExpanderHeader {{
            color: {t['text_color']} !important;
            font-weight: bold;
        }}
        </style>
        """, unsafe_allow_html=True)