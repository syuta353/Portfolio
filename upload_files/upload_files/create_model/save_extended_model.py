import os
import json
import re
import joblib
import pandas as pd
from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.ensemble import VotingClassifier, RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.neural_network import MLPClassifier

class DeviceModelTrainer:
    def __init__(self, json_path, model_dir="device_models"):
        # JSONãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã¨ãƒ¢ãƒ‡ãƒ«ä¿å­˜å…ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’åˆæœŸåŒ–
        self.json_path = json_path
        self.model_dir = model_dir
        os.makedirs(self.model_dir, exist_ok=True)

    def load_json(self):
        # JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€
        with open(self.json_path, "r", encoding="utf-8") as f:
            return json.load(f)

    def clean_text(self, text):
        # æ”¹è¡Œã‚„ã‚¿ãƒ–ã‚’ã‚¹ãƒšãƒ¼ã‚¹ã«ç½®æ›ã—ã€å‰å¾Œã®ç©ºç™½ã‚’é™¤å»
        if pd.isna(text):
            return ""
        return str(text).replace("\n", " ").replace("\t", " ").strip()

    def extract_error_codes(self, text):
        # 7æ¡ã®æ•°å­—ï¼ˆã‚¨ãƒ©ãƒ¼ã‚³ãƒ¼ãƒ‰ï¼‰ã‚’æŠ½å‡º
        return " ".join(re.findall(r"\b\d{7}\b", text))

    def build_error_mapping(self, df):
        # ã‚¨ãƒ©ãƒ¼ã‚³ãƒ¼ãƒ‰ãƒ»äº¤æ›éƒ¨å“ãƒ»è£…ç½®ç¨®åˆ¥ã®å¯¾å¿œè¡¨ã‚’æ§‹ç¯‰
        records = []
        for _, row in df.iterrows():
            error_text = str(row.get("éšœå®³ã‚¨ãƒ©ãƒ¼åæŠ½å‡º", ""))
            parts_text = str(row.get("äº¤æ›éƒ¨å“", ""))
            device_type = str(row.get("è£…ç½®ç¨®åˆ¥", ""))
            error_codes = set(re.findall(r"\b\d{7}\b", error_text))
            split_parts = str(parts_text).splitlines()
            split_parts = [p.strip() for p in split_parts if p.strip()]
            for code in error_codes:
                for part in split_parts:
                    records.append({
                        "ã‚¨ãƒ©ãƒ¼ã‚³ãƒ¼ãƒ‰": code,
                        "äº¤æ›éƒ¨å“": part,
                        "è£…ç½®ç¨®åˆ¥": device_type
                    })
        return pd.DataFrame(records)

    def train_and_save_models(self):
        # ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰ã¨ä¿å­˜ã®ãƒ¡ã‚¤ãƒ³å‡¦ç†
        json_data = self.load_json()
        df = pd.DataFrame(json_data)

        # ä¸å…·åˆå†…å®¹ã®å‰å‡¦ç†ã¨ã‚¨ãƒ©ãƒ¼ã‚³ãƒ¼ãƒ‰æŠ½å‡º
        df["ä¸å…·åˆå†…å®¹_cleaned"] = df["ä¸å…·åˆå†…å®¹"].apply(self.clean_text)
        df["éšœå®³ã‚¨ãƒ©ãƒ¼åæŠ½å‡º"] = df["ä¸å…·åˆå†…å®¹_cleaned"].apply(self.extract_error_codes)

        # ã‚¨ãƒ©ãƒ¼ã‚³ãƒ¼ãƒ‰ãƒ»è£…ç½®ç¨®åˆ¥ãƒ»äº¤æ›éƒ¨å“ã®å¯¾å¿œè¡¨ã‚’ä½œæˆ
        error_df = self.build_error_mapping(df)

        for device in error_df["è£…ç½®ç¨®åˆ¥"].unique():
            subset = error_df[error_df["è£…ç½®ç¨®åˆ¥"] == device].copy()
            sample_counts = subset["äº¤æ›éƒ¨å“"].value_counts().to_dict()

            if len(subset["äº¤æ›éƒ¨å“"].unique()) < 2:
                continue

            # ãƒ©ãƒ™ãƒ«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
            le_code = LabelEncoder()
            le_parts = LabelEncoder()
            subset["error_code_encoded"] = le_code.fit_transform(subset["ã‚¨ãƒ©ãƒ¼ã‚³ãƒ¼ãƒ‰"])
            subset["parts_encoded"] = le_parts.fit_transform(subset["äº¤æ›éƒ¨å“"])

            # ç‰¹å¾´é‡ã¨ç›®çš„å¤‰æ•°
            X = subset[["error_code_encoded"]]
            y = subset["parts_encoded"]

            # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
            scaler = StandardScaler()
            X_scaled = scaler.fit_transform(X)

            # å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¨æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã«åˆ†å‰²
            X_train, _, y_train, _ = train_test_split(X_scaled, y, test_size=0.3, random_state=42)
            knn_n = min(5, len(X_train))

            # ã‚¯ãƒ©ã‚¹ãŒ1ã¤ã—ã‹ãªã„å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
            if len(set(y_train)) < 2:
                continue

            # ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰
            model = VotingClassifier(
                estimators=[
                    ('rf', RandomForestClassifier(random_state=42)),
                    ('lr', LogisticRegression(max_iter=1000, class_weight='balanced')),
                    ('knn', KNeighborsClassifier(n_neighbors=knn_n)),
                    ('mlp', MLPClassifier(hidden_layer_sizes=(100,), max_iter=1000, learning_rate_init=0.001))
                ],
                voting='soft'
            )
            model.fit(X_train, y_train)

            # ä¿å­˜
            safe_device = re.sub(r"[\\/:*?\"<>|]", "_", device)
            joblib.dump(model, os.path.join(self.model_dir, f"model_{safe_device}.pkl"))
            joblib.dump(le_code, os.path.join(self.model_dir, f"le_code_{safe_device}.pkl"))
            joblib.dump(le_parts, os.path.join(self.model_dir, f"le_parts_{safe_device}.pkl"))
            joblib.dump(scaler, os.path.join(self.model_dir, f"scaler_{safe_device}.pkl"))
            joblib.dump(sample_counts, os.path.join(self.model_dir, f"sample_counts_{safe_device}.pkl"))

            # ä¿å­˜å†…å®¹ã®ç¢ºèªå‡ºåŠ›
            print(f"\nğŸ“¦ è£…ç½®ç¨®åˆ¥: {device}")
            print("ãƒ¢ãƒ‡ãƒ«æ§‹æˆ:", model)
            print("ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰å¯¾è±¡ã‚¨ãƒ©ãƒ¼ã‚³ãƒ¼ãƒ‰:", list(le_code.classes_))
            print("ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰å¯¾è±¡äº¤æ›éƒ¨å“:", list(le_parts.classes_))
            print("ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼å¹³å‡:", scaler.mean_)
            
        print("âœ… Fin")

# å®Ÿè¡Œä¾‹
if __name__ == "__main__":
    trainer = DeviceModelTrainer(json_path="failed_db.json")
    trainer.train_and_save_models()