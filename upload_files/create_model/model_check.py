import streamlit as st
import pandas as pd
import json
import os
import re
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import matplotlib
from janome.tokenizer import Tokenizer
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.ensemble import RandomForestClassifier, VotingClassifier, StackingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.neural_network import MLPClassifier
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import classification_report, confusion_matrix, precision_recall_curve
from sklearn.preprocessing import label_binarize

from sklearn.naive_bayes import MultinomialNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.ensemble import VotingClassifier
from sklearn.preprocessing import StandardScaler

st.title("ğŸ” ãƒ¢ãƒ‡ãƒ«è©•ä¾¡ãƒ„ãƒ¼ãƒ«")
matplotlib.rcParams['font.family'] = 'MS Gothic'

# ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã¨å‰å‡¦ç†
def load_json(path):
    with open(path, "r", encoding="utf-8") as f:
        return json.load(f)

def clean_text(text):
    if pd.isna(text):
        return ""
    return str(text).replace("\n", " ").replace("\t", " ").strip()

def build_error_mapping(df):
    records = []
    for _, row in df.iterrows():
        error_text = str(row.get("éšœå®³ã‚¨ãƒ©ãƒ¼åæŠ½å‡º", ""))
        parts_text = str(row.get("äº¤æ›éƒ¨å“", ""))
        device_type = str(row.get("è£…ç½®ç¨®åˆ¥", ""))
        error_codes = set(re.findall(r"\b\d{7}\b", error_text))
        for code in error_codes:
            records.append({
                "ã‚¨ãƒ©ãƒ¼ã‚³ãƒ¼ãƒ‰": code,
                "äº¤æ›éƒ¨å“": parts_text,
                "è£…ç½®ç¨®åˆ¥": device_type
            })
    return pd.DataFrame(records)

# ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
json_data = load_json("failed_db.json")
df = pd.DataFrame(json_data)
df["ä¸å…·åˆå†…å®¹_cleaned"] = df["ä¸å…·åˆå†…å®¹"].apply(clean_text)

# ã‚¨ãƒ©ãƒ¼åæŠ½å‡ºï¼ˆç°¡æ˜“ç‰ˆï¼š7æ¡æ•°å­—ã®ã¿ï¼‰
df["éšœå®³ã‚¨ãƒ©ãƒ¼åæŠ½å‡º"] = df["ä¸å…·åˆå†…å®¹_cleaned"].apply(lambda x: " ".join(re.findall(r"\b\d{7}\b", x)))

# error_dfæ§‹ç¯‰
error_df = build_error_mapping(df)

# ãƒ©ãƒ™ãƒ«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
le_code = LabelEncoder()
le_device = LabelEncoder()
le_parts = LabelEncoder()

error_df["error_code_encoded"] = le_code.fit_transform(error_df["ã‚¨ãƒ©ãƒ¼ã‚³ãƒ¼ãƒ‰"])
error_df["device_type_encoded"] = le_device.fit_transform(error_df["è£…ç½®ç¨®åˆ¥"])
error_df["parts_encoded"] = le_parts.fit_transform(error_df["äº¤æ›éƒ¨å“"])

# ç‰¹å¾´é‡ã¨ç›®çš„å¤‰æ•°
X = error_df[["error_code_encoded", "device_type_encoded"]]
y = error_df["parts_encoded"]
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# ãƒ‡ãƒ¼ã‚¿åˆ†å‰²
X_train, _, y_train, _ = train_test_split(X_scaled, y, test_size=0.3, random_state=42)
# ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰ï¼ˆmax_iter å¢—åŠ ï¼‰
rf = RandomForestClassifier(random_state=42)
lr = LogisticRegression(max_iter=1000, class_weight='balanced')
nb = MultinomialNB()
knn = KNeighborsClassifier(n_neighbors=5)
mlp = MLPClassifier(hidden_layer_sizes=(100,), max_iter=1000, learning_rate_init=0.001)

extended_voting = VotingClassifier(
    estimators=[('rf', rf), ('lr', lr), ('knn', knn), ('mlp', mlp)],
    voting='soft'
)

# å­¦ç¿’
extended_voting.fit(X_train, y_train)


def show_evaluation(name, model, X_test, y_test, le_parts):
    y_pred = model.predict(X_test)
    labels = np.unique(y_test)
    st.markdown(f"### ğŸ“Š {name} ãƒ¢ãƒ‡ãƒ«è©•ä¾¡")
    st.text(classification_report(y_test, y_pred, labels=labels, target_names=le_parts.inverse_transform(labels)))
    return y_pred
    
def plot_confusion_matrix(y_true, y_pred, le_parts, title):
    cm = confusion_matrix(y_true, y_pred)
    fig, ax = plt.subplots(figsize=(10, 8))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax,
                xticklabels=le_parts.inverse_transform(np.unique(y_true)),
                yticklabels=le_parts.inverse_transform(np.unique(y_true)))
    ax.set_title(title)
    ax.set_xlabel("äºˆæ¸¬ã•ã‚ŒãŸéƒ¨å“")
    ax.set_ylabel("å®Ÿéš›ã®éƒ¨å“")
    st.pyplot(fig)
    return cm

def show_top_misclassified(cm, le_parts):
    cm_df = pd.DataFrame(cm,
                         index=le_parts.inverse_transform(np.unique(y_test)),
                         columns=le_parts.inverse_transform(np.unique(y_test)))
    misclassified = cm_df.copy()
    np.fill_diagonal(misclassified.values, 0)
    misclassified_sum = misclassified.sum(axis=1).sort_values(ascending=False)
    st.markdown("### âŒ èª¤åˆ†é¡ã®å¤šã„éƒ¨å“ãƒˆãƒƒãƒ—10")
    st.dataframe(misclassified_sum.head(10))
    
def plot_pr_curve(model, X_test, y_test, le_parts,title):
    y_test_bin = label_binarize(y_test, classes=model.classes_)
    y_score = model.predict_proba(X_test)
    fig, ax = plt.subplots(figsize=(10, 6))
    for i in range(y_score.shape[1]):
        precision, recall, _ = precision_recall_curve(y_test_bin[:, i], y_score[:, i])
        label = le_parts.inverse_transform([model.classes_[i]])[0]
        ax.plot(recall, precision, label=f"Class {label}")
    ax.set_xlabel("Recall")
    ax.set_ylabel("Precision")
    ax.set_title(title + "ã€€" + "Precision-Recall Curve")
    ax.legend()
    st.pyplot(fig)
   

# ==============================
# ğŸ“ˆ çµ±è¨ˆè¡¨ç¤º
# ==============================
st.markdown("### ğŸ“ˆ ãƒ‡ãƒ¼ã‚¿çµ±è¨ˆæƒ…å ±")
st.write("ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªã‚¨ãƒ©ãƒ¼ã‚³ãƒ¼ãƒ‰:", error_df["ã‚¨ãƒ©ãƒ¼ã‚³ãƒ¼ãƒ‰"].nunique())
st.write("ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªè£…ç½®ç¨®åˆ¥:", error_df["è£…ç½®ç¨®åˆ¥"].nunique())
st.write("ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªäº¤æ›éƒ¨å“:", error_df["äº¤æ›éƒ¨å“"].nunique())

# è©•ä¾¡è¡¨ç¤º
y_pred_extended = extended_voting.predict(X_train)
st.markdown("### ğŸ“Š Extended VotingClassifier ãƒ¢ãƒ‡ãƒ«è©•ä¾¡")
st.text(classification_report(y_train, y_pred_extended, target_names=le_parts.inverse_transform(np.unique(y_train))))

# æ··åŒè¡Œåˆ—ã¨èª¤åˆ†é¡åˆ†æ
st.markdown("### ğŸ” å„ãƒ¢ãƒ‡ãƒ«ã®æ··åŒè¡Œåˆ—")
stack_cm = plot_confusion_matrix(y_train, y_pred_extended, le_parts, "Extended VotingClassifier ã®æ··åŒè¡Œåˆ—")

# Precision-Recall æ›²ç·š
st.markdown("### ğŸ“‰ Precision-Recall æ›²ç·š")
plot_pr_curve(extended_voting, X_train, y_train, le_parts, "Extended VotingClassifier")


import joblib

st.markdown("### ğŸ” è£…ç½®ç¨®åˆ¥ã”ã¨ã®ãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã‚‹äº¤æ›éƒ¨å“äºˆæ¸¬")

device_type_str = st.text_input("è£…ç½®ç¨®åˆ¥ã‚’å…¥åŠ›")
error_code = st.number_input("ã‚¨ãƒ©ãƒ¼ã‚³ãƒ¼ãƒ‰ï¼ˆ7æ¡ï¼‰ã‚’å…¥åŠ›", min_value=1000000, max_value=9999999, step=1)
error_code_str = str(error_code)  # æ•°å€¤ã‚’æ–‡å­—åˆ—ã«å¤‰æ›

if st.button("äºˆæ¸¬ã™ã‚‹"):
    safe_device = re.sub(r"[^\w\-]", "_", device_type_str)
    try:
        model_path = f"model/device_models/model_{safe_device}.pkl"
        le_code_path = f"model/device_models/le_code_{safe_device}.pkl"
        le_parts_path = f"model/device_models/le_parts_{safe_device}.pkl"
        scaler_path = f"model/device_models/scaler_{safe_device}.pkl"

        model = joblib.load(model_path)
        le_code = joblib.load(le_code_path)
        le_parts = joblib.load(le_parts_path)
        scaler = joblib.load(scaler_path)

        if error_code_str not in le_code.classes_:
            st.error("æŒ‡å®šã•ã‚ŒãŸã‚¨ãƒ©ãƒ¼ã‚³ãƒ¼ãƒ‰ã¯ã“ã®è£…ç½®ç¨®åˆ¥ã«å­˜åœ¨ã—ã¾ã›ã‚“ã€‚")
        else:
            code_encoded = le_code.transform([error_code_str])
            X_input = scaler.transform([[code_encoded[0]]])
            proba = model.predict_proba(X_input)[0]
            part_indices = proba.argsort()[::-1]

            st.markdown("#### ğŸ”§ äºˆæ¸¬ã•ã‚Œã‚‹äº¤æ›éƒ¨å“ã¨ãã®ç¢ºç‡")
            for idx in part_indices:
                percent = round(proba[idx] * 100, 2)
                if percent > 0.0:
                    part_name = le_parts.inverse_transform([idx])[0]
                    st.write(f"- {part_name}: {percent}%")
    except Exception as e:
        st.error(f"ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ã¾ãŸã¯äºˆæ¸¬ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")